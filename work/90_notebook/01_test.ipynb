{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# テストの実行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "import subprocess\n",
    "import logging\n",
    "from concurrent import futures\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "TESTSET_DIR = os.path.join('/home', 'jovyan', 'work', '01_testset')\n",
    "PRJ_DIR = os.path.join('/home', 'jovyan', 'work')\n",
    "\n",
    "PROG_PATH = os.path.join(PRJ_DIR, 'main')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## マスタの読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_seed_df = pd.read_csv(os.path.join(TESTSET_DIR, '01_testset_pre_master.csv'), usecols=['seed'])\n",
    "sys_seed_df = pd.read_csv(os.path.join(TESTSET_DIR, '02_testset_sys_master.csv'), usecols=['seed'])\n",
    "stress_seed_df = pd.read_csv(os.path.join(TESTSET_DIR, '03_testset_stress_master.csv'), usecols=['seed'])\n",
    "param_seed_df = pd.read_csv(os.path.join(TESTSET_DIR, '04_testset_param_master.csv'), usecols=['seed'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 相対スコア用にChampionDataの読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    CHAMP_TAG = 'yyyymmdd_hhmm'\n",
    "    CHAMP_DIR = os.path.join('/home', 'jovyan', 'work', 'result', 'champion')\n",
    "\n",
    "    champ_path = os.path.join(CHAMP_DIR, 'champ_all_{}.csv'.format(CHAMP_TAG))\n",
    "    champ_df = pd.read_csv(champ_path)\n",
    "\n",
    "    top_rate = 1.00  # 順位表を参考にチャンピオンスコアを補正\n",
    "    champ_score_dict = {}\n",
    "\n",
    "    for _, row in champ_df.iterrows():\n",
    "        seed = row['seed']\n",
    "        score = row['champion_score']\n",
    "\n",
    "        champ_score_dict[seed] = score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 実行するロジックの指定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 実行プログラムにタグをつけておく\n",
    "## Champion管理用に単語は -(ハイフン) で区切る\n",
    "PROG_TAG = 'first-prog'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve(seed):\n",
    "    problem_path = os.path.join(TESTSET_DIR, 'in', '{:0>4}.txt'.format(seed)) \n",
    "    command_str = 'echo {} | {}'.format(problem_path, PROG_PATH)\n",
    "\n",
    "    # stack overflow対策\n",
    "    # command_str = 'ulimit -S -s 1048576 && echo {} | {}'.format(problem_path, PROG_PATH)\n",
    "\n",
    "    start_time = time.perf_counter()\n",
    "    res = subprocess.run(command_str, stdout=subprocess.PIPE, stderr=subprocess.PIPE, shell=True)\n",
    "\n",
    "    # 経過時間(ミリ秒単位)\n",
    "    e_time = time.perf_counter() - start_time\n",
    "    e_time = int(1000 * e_time)    \n",
    "    \n",
    "    #print('{}'.format(prob_id))    \n",
    "    return (seed, e_time, res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_test(testset_name):\n",
    "    result_df = pd.DataFrame()\n",
    "    future_list = []\n",
    "\n",
    "    logger.info('Start')\n",
    "\n",
    "    testset_path = os.path.join(TESTSET_DIR, testset_name + '_master.csv')\n",
    "    testset_df = pd.read_csv(testset_path)\n",
    "\n",
    "    # 24並列実行\n",
    "    with futures.ThreadPoolExecutor(max_workers=24) as executor:\n",
    "        seed_list = testset_df['seed'].to_list()\n",
    "        future_list = list(tqdm(executor.map(solve, seed_list), total=len(seed_list)))\n",
    "\n",
    "    for future in future_list:\n",
    "        seed, e_time, res = future\n",
    "\n",
    "        # 結果をまとめる\n",
    "        solve_result = []\n",
    "        \n",
    "        solve_result.append(testset_name)\n",
    "\n",
    "        # 問題パラメタ\n",
    "        solve_result.append(seed)\n",
    "\n",
    "        # 経過時間\n",
    "        solve_result.append(e_time)\n",
    "        \n",
    "        try:\n",
    "            # -- start -- 生成コード貼り付け先\n",
    "            elem_cnt = 2\n",
    "\n",
    "            result = str(res.stderr.decode('utf-8').split()[-elem_cnt + 0].replace('Result=', ''))\n",
    "            score = int(res.stderr.decode('utf-8').split()[-elem_cnt + 1].replace('Score=', ''))\n",
    "\n",
    "            solve_result.append(score)\n",
    "            solve_result.append(result)   \n",
    "            # -- end -- 生成コード貼り付け先\n",
    "\n",
    "            # 相対スコア\n",
    "            # rel_score = int(10 ** 9 * top_rate * champ_score_dict[seed] / score)\n",
    "            # solve_result.append(rel_score)\n",
    "\n",
    "        except Exception as e:\n",
    "            print('Error: seed={}'.format(seed))\n",
    "            print(e)\n",
    "            return\n",
    "\n",
    "        result_df = pd.concat([result_df, pd.DataFrame(solve_result).T], axis=0)\n",
    "\n",
    "    logger.info('finish!')\n",
    "    \n",
    "    # 結果を整形\n",
    "    result_df.index = range(result_df.shape[0])\n",
    "    cols = ['testset', 'seed', 'time', 'score', 'result']\n",
    "    result_df.columns = cols\n",
    "\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "fmt = \"%(asctime)s: %(message)s\"\n",
    "logging.basicConfig(level=logging.INFO, format=fmt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_summary_df(result_df):\n",
    "    # 全体サマリ\n",
    "    summary_all_df = pd.DataFrame()\n",
    "\n",
    "    for testset in np.unique(result_df['testset']):\n",
    "        test_result_df = result_df.query('testset == \"{}\"'.format(testset))\n",
    "\n",
    "        summary_df = pd.DataFrame(\n",
    "        {\n",
    "            'testset': [testset],\n",
    "            \n",
    "            'time_mean': [int(np.mean(test_result_df['time']))],\n",
    "            \n",
    "            # -- start -- 生成コード貼り付け先\n",
    "            'score_mean': [np.mean(test_result_df['score'])],\n",
    "            'score_min': [min(test_result_df['score'])],\n",
    "            'score_max': [max(test_result_df['score'])],\n",
    "            # -- end -- 生成コード貼り付け先\n",
    "\n",
    "            'time_max': [max(test_result_df['time'])],\n",
    "        })\n",
    "\n",
    "        summary_all_df = pd.concat([summary_all_df, summary_df], axis=0)   \n",
    "\n",
    "    summary_all_df['tag'] = PROG_TAG\n",
    "    \n",
    "    cols = ['tag']\n",
    "    cols.extend(summary_df.columns)\n",
    "    \n",
    "    summary_all_df = summary_all_df[cols]\n",
    "    \n",
    "    return summary_all_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-24 15:58:30,117: Start\n",
      "2023-06-24 15:58:30,385: finish!\n"
     ]
    }
   ],
   "source": [
    "PROG_NAME_LIST = ['main']\n",
    "#PROG_NAME_LIST = ['main', 'main_off']\n",
    "\n",
    "testset_name = '01_testset_pre'\n",
    "#testset_name = '02_testset_sys'\n",
    "#testset_name = '03_testset_stress'\n",
    "#testset_name = '04_testset_param'\n",
    "\n",
    "result_dict = {}\n",
    "summary_all_dict = {}\n",
    "\n",
    "for PROG_NAME in PROG_NAME_LIST:\n",
    "    prog_path = os.path.join(PRJ_DIR, PROG_NAME)\n",
    "    \n",
    "    result_df = pd.DataFrame()\n",
    "    \n",
    "    testset_result_df = run_test(testset_name)\n",
    "    result_df = pd.concat([result_df, testset_result_df], axis=0)\n",
    "    \n",
    "    result_dict[PROG_NAME] = result_df\n",
    "    summary_all_dict[PROG_NAME] = get_summary_df(result_df)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tag</th>\n",
       "      <th>testset</th>\n",
       "      <th>time_mean</th>\n",
       "      <th>score_mean</th>\n",
       "      <th>score_min</th>\n",
       "      <th>score_max</th>\n",
       "      <th>time_max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>first-prog</td>\n",
       "      <td>01_test_pre</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          tag      testset  time_mean  score_mean  score_min  score_max  \\\n",
       "0  first-prog  01_test_pre         10         0.0          0          0   \n",
       "\n",
       "   time_max  \n",
       "0        21  "
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pre test\n",
    "result_sub_df = pd.merge(pre_seed_df, result_df, on='seed')\n",
    "summary_df = get_summary_df(result_sub_df)\n",
    "summary_df['testset'] = '01_test_pre'\n",
    "\n",
    "summary_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sys param\n",
    "if testset_name == '04_testset_param' or testset_name == '02_testset_sys' or testset_name == '03_testset_stress':\n",
    "    summary_df = pd.DataFrame()\n",
    "    \n",
    "    result_sub_df = pd.merge(param_seed_df, result_df, on='seed')\n",
    "    summary_df = get_summary_df(result_sub_df)\n",
    "    summary_df['testset'] = '04_test_param'\n",
    "\n",
    "summary_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sys test\n",
    "if testset_name == '02_testset_sys' or testset_name == '03_testset_stress':\n",
    "    summary_df = pd.DataFrame()\n",
    "    \n",
    "    result_sub_df = pd.merge(sys_seed_df, result_df, on='seed')\n",
    "    summary_df = get_summary_df(result_sub_df)\n",
    "    summary_df['testset'] = '02_test_sys'\n",
    "\n",
    "summary_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stress test\n",
    "if testset_name == '03_testset_stress':\n",
    "    summary_df = pd.DataFrame()\n",
    "    \n",
    "    result_sub_df = pd.merge(stress_seed_df, result_df, on='seed')\n",
    "    summary_df = get_summary_df(result_sub_df)\n",
    "    summary_df['testset'] = '03_test_stress'\n",
    "\n",
    "summary_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>testset</th>\n",
       "      <th>seed</th>\n",
       "      <th>time</th>\n",
       "      <th>score</th>\n",
       "      <th>result</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>01_testset_pre</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>dummy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>01_testset_pre</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>dummy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>01_testset_pre</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>dummy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>01_testset_pre</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>dummy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>01_testset_pre</td>\n",
       "      <td>70</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>dummy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>01_testset_pre</td>\n",
       "      <td>40</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>dummy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>01_testset_pre</td>\n",
       "      <td>39</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>dummy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>01_testset_pre</td>\n",
       "      <td>33</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>dummy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>01_testset_pre</td>\n",
       "      <td>48</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>dummy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>01_testset_pre</td>\n",
       "      <td>148</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>dummy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>300 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            testset seed time score result\n",
       "1    01_testset_pre    1    3     0  dummy\n",
       "3    01_testset_pre    3    3     0  dummy\n",
       "9    01_testset_pre    9    3     0  dummy\n",
       "10   01_testset_pre   10    3     0  dummy\n",
       "70   01_testset_pre   70    4     0  dummy\n",
       "..              ...  ...  ...   ...    ...\n",
       "40   01_testset_pre   40   19     0  dummy\n",
       "39   01_testset_pre   39   19     0  dummy\n",
       "33   01_testset_pre   33   19     0  dummy\n",
       "48   01_testset_pre   48   20     0  dummy\n",
       "148  01_testset_pre  148   21     0  dummy\n",
       "\n",
       "[300 rows x 5 columns]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df.sort_values('time', ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#result_df.sort_values('rel_score', ascending=True).head(n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 結果ログの保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_now = datetime.datetime.now() + datetime.timedelta(hours=9)\n",
    "time_str = t_now.strftime('%Y%m%d_%H%M')\n",
    "\n",
    "for PROG_NAME in PROG_NAME_LIST:\n",
    "    result_df = result_dict[PROG_NAME]\n",
    "    \n",
    "    for testset in np.unique(result_df['testset']):\n",
    "        csv_df = result_df.query('testset == \"{}\"'.format(testset))\n",
    "        csv_df.to_csv(PRJ_DIR+'/result/{}_{}_{}_{}.csv'.format(time_str,PROG_TAG, testset, PROG_NAME), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! cp $PRJ_DIR/main $PRJ_DIR/result/bin/$PROG_TAG"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
