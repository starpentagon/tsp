{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# テストの実行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "import subprocess\n",
    "import logging\n",
    "from concurrent import futures\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "TESTSET_DIR = os.path.join('/home', 'jovyan', 'work', '01_testset')\n",
    "PRJ_DIR = os.path.join('/home', 'jovyan', 'work')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## マスタの読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_seed_df = pd.read_csv(os.path.join(TESTSET_DIR, '01_testset_pre_master.csv'), usecols=['seed'])\n",
    "#sys_seed_df = pd.read_csv(os.path.join(TESTSET_DIR, '02_testset_sys_master.csv'), usecols=['seed'])\n",
    "#stress_seed_df = pd.read_csv(os.path.join(TESTSET_DIR, '03_testset_stress_master.csv'), usecols=['seed'])\n",
    "#param_seed_df = pd.read_csv(os.path.join(TESTSET_DIR, '04_testset_param_master.csv'), usecols=['seed'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 相対スコア用にChampionDataの読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    CHAMP_TAG = 'yyyymmdd_hhmm'\n",
    "    CHAMP_DIR = os.path.join('/home', 'jovyan', 'work', 'result', 'champion')\n",
    "\n",
    "    champ_path = os.path.join(CHAMP_DIR, 'champ_all_{}.csv'.format(CHAMP_TAG))\n",
    "    champ_df = pd.read_csv(champ_path)\n",
    "\n",
    "    top_rate = 1.00  # 順位表を参考にチャンピオンスコアを補正\n",
    "    champ_score_dict = {}\n",
    "\n",
    "    for _, row in champ_df.iterrows():\n",
    "        seed = row['seed']\n",
    "        score = row['champion_score']\n",
    "\n",
    "        champ_score_dict[seed] = score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 実行するロジックの指定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 実行プログラムにタグをつけておく\n",
    "## Champion管理用に単語は -(ハイフン) で区切る\n",
    "PROG_TAG = 'first-prog'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve(seed, prog_path):\n",
    "    problem_path = os.path.join(TESTSET_DIR, 'in', '{:0>4}.txt'.format(seed)) \n",
    "    command_str = 'echo {} | {}'.format(problem_path, prog_path)\n",
    "\n",
    "    start_time = time.perf_counter()\n",
    "    res = subprocess.run(command_str, stdout=subprocess.PIPE, stderr=subprocess.PIPE, shell=True)\n",
    "\n",
    "    # 経過時間(ミリ秒単位)\n",
    "    e_time = time.perf_counter() - start_time\n",
    "    e_time = int(1000 * e_time)    \n",
    "    \n",
    "    #print('{}'.format(prob_id))    \n",
    "    return (seed, e_time, res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run_test用のコード生成\n",
    "elem_list = [\n",
    "    ('Result', 'str'),\n",
    "    ('Score', 'int'),\n",
    "    ]\n",
    "\n",
    "if True:\n",
    "    # Parse   \n",
    "    tab_str = ' ' * 12\n",
    "    \n",
    "    print(tab_str + 'elem_cnt = {}'.format(len(elem_list)))\n",
    "    print()\n",
    "    \n",
    "    for i in range(len(elem_list)):\n",
    "        elem, elem_type = elem_list[i]\n",
    "        elem_lower = elem.lower()\n",
    "        \n",
    "        parse_str = \"{} = {}(res.stderr.decode('utf-8').split()[-elem_cnt + {}].replace('{}=', ''))\".format(elem_lower, elem_type, i, elem)\n",
    "        print(tab_str + parse_str)\n",
    "        \n",
    "    print()\n",
    "    \n",
    "    # solve_resultへの格納\n",
    "    format_elem_list = []\n",
    "    \n",
    "    for elem, elem_type in elem_list:\n",
    "        if elem == \"Result\":\n",
    "            continue\n",
    "            \n",
    "        format_elem_list.append(elem.lower())\n",
    "    \n",
    "    format_elem_list.append(\"result\")\n",
    "    \n",
    "    for elem in format_elem_list:\n",
    "        append_str = \"solve_result.append({})\".format(elem)\n",
    "        print(tab_str + append_str)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 結果の整形用コード生成\n",
    "if True:\n",
    "    col_str = \"    cols = ['testset', 'seed', 'time'\"\n",
    "    \n",
    "    for elem in format_elem_list:\n",
    "        col_str += \", '{}'\".format(elem)\n",
    "    col_str += \"]\"\n",
    "    \n",
    "    print(col_str)\n",
    "    print(\"    result_df.columns = cols\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_test(testset_name):\n",
    "    result_df = pd.DataFrame()\n",
    "    future_list = []\n",
    "\n",
    "    logger.info('Start')\n",
    "\n",
    "    testset_path = os.path.join(TESTSET_DIR, testset_name + '_master.csv')\n",
    "    testset_df = pd.read_csv(testset_path)\n",
    "\n",
    "    # 24並列実行\n",
    "    with futures.ThreadPoolExecutor(max_workers=24) as executor:\n",
    "        for _, row in testset_df.iterrows():\n",
    "            seed = row\n",
    "            \n",
    "            # バッチ実行\n",
    "            future = executor.submit(solve, seed=seed, prog_path=prog_path)\n",
    "            future_list.append(future)\n",
    "\n",
    "        _ = futures.as_completed(fs=future_list)\n",
    "\n",
    "    for future in future_list:\n",
    "        seed, e_time, res = future.result()\n",
    "\n",
    "        # 結果をまとめる\n",
    "        solve_result = []\n",
    "        \n",
    "        solve_result.append(testset_name)\n",
    "\n",
    "        # 問題パラメタ\n",
    "        solve_result.append(seed)\n",
    "\n",
    "        # 経過時間\n",
    "        solve_result.append(e_time)\n",
    "        \n",
    "        try:\n",
    "            # -- start -- 生成コード貼り付け先\n",
    "            elem_cnt = 2\n",
    "\n",
    "            result = str(res.stderr.decode('utf-8').split()[-elem_cnt + 0].replace('Result=', ''))\n",
    "            score = int(res.stderr.decode('utf-8').split()[-elem_cnt + 1].replace('Score=', ''))\n",
    "\n",
    "            solve_result.append(score)\n",
    "            solve_result.append(result)   \n",
    "            # -- end -- 生成コード貼り付け先\n",
    "\n",
    "            # 相対スコア\n",
    "            # rel_score = int(10 ** 9 * top_rate * champ_score_dict[seed] / score)\n",
    "            # solve_result.append(rel_score)\n",
    "\n",
    "        except Exception as e:\n",
    "            print('Error: seed={}'.format(seed))\n",
    "            print(e)\n",
    "            return\n",
    "\n",
    "        result_df = pd.concat([result_df, pd.DataFrame(solve_result).T], axis=0)\n",
    "\n",
    "    logger.info('finish!')\n",
    "    \n",
    "    # 結果を整形\n",
    "    result_df.index = range(result_df.shape[0])\n",
    "    cols = ['testset', 'seed', 'time', 'score', 'result']\n",
    "    result_df.columns = cols\n",
    "\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "fmt = \"%(asctime)s: %(message)s\"\n",
    "logging.basicConfig(level=logging.INFO, format=fmt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            'score_mean': [np.mean(test_result_df['score'])],\n",
      "            'score_min': [min(test_result_df['score'])],\n",
      "            'score_max': [max(test_result_df['score'])],\n"
     ]
    }
   ],
   "source": [
    "# 全体サマリ用コード\n",
    "elem_list = [\n",
    "    ('score', 'mean', 'np.mean'),\n",
    "    ('score', 'min', 'min'),\n",
    "    ('score', 'max', 'max'),\n",
    "    ]\n",
    "\n",
    "if True:\n",
    "    tab_str = ' ' * 12\n",
    "    \n",
    "    for col, suffix, oper in elem_list:\n",
    "        kv_str = \"'{}_{}': [{}(test_result_df['{}'])],\".format(col, suffix, oper, col)\n",
    "        print(tab_str + kv_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_summary_df(result_df):\n",
    "    # 全体サマリ\n",
    "    summary_all_df = pd.DataFrame()\n",
    "\n",
    "    for testset in np.unique(result_df['testset']):\n",
    "        test_result_df = result_df.query('testset == \"{}\"'.format(testset))\n",
    "\n",
    "        summary_df = pd.DataFrame(\n",
    "        {\n",
    "            'testset': [testset],\n",
    "            \n",
    "            'time_mean': [int(np.mean(test_result_df['time']))],\n",
    "            \n",
    "            # -- start -- 生成コード貼り付け先\n",
    "            'score_mean': [np.mean(test_result_df['score'])],\n",
    "            'score_min': [min(test_result_df['score'])],\n",
    "            'score_max': [max(test_result_df['score'])],\n",
    "            # -- end -- 生成コード貼り付け先\n",
    "\n",
    "            'time_max': [max(test_result_df['time'])],\n",
    "        })\n",
    "\n",
    "        summary_all_df = pd.concat([summary_all_df, summary_df], axis=0)   \n",
    "\n",
    "    summary_all_df['tag'] = PROG_TAG\n",
    "    \n",
    "    cols = ['tag']\n",
    "    cols.extend(summary_df.columns)\n",
    "    \n",
    "    summary_all_df = summary_all_df[cols]\n",
    "    \n",
    "    return summary_all_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-24 15:58:30,117: Start\n",
      "2023-06-24 15:58:30,385: finish!\n"
     ]
    }
   ],
   "source": [
    "PROG_NAME_LIST = ['main']\n",
    "#PROG_NAME_LIST = ['main', 'main_off']\n",
    "\n",
    "testset_name = '01_testset_pre'\n",
    "#testset_name = '02_testset_sys'\n",
    "#testset_name = '03_testset_stress'\n",
    "#testset_name = '04_testset_param'\n",
    "\n",
    "result_dict = {}\n",
    "summary_all_dict = {}\n",
    "\n",
    "for PROG_NAME in PROG_NAME_LIST:\n",
    "    prog_path = os.path.join(PRJ_DIR, PROG_NAME)\n",
    "    \n",
    "    result_df = pd.DataFrame()\n",
    "    \n",
    "    testset_result_df = run_test(testset_name)\n",
    "    result_df = pd.concat([result_df, testset_result_df], axis=0)\n",
    "    \n",
    "    result_dict[PROG_NAME] = result_df\n",
    "    summary_all_dict[PROG_NAME] = get_summary_df(result_df)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tag</th>\n",
       "      <th>testset</th>\n",
       "      <th>time_mean</th>\n",
       "      <th>score_mean</th>\n",
       "      <th>score_min</th>\n",
       "      <th>score_max</th>\n",
       "      <th>time_max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>first-prog</td>\n",
       "      <td>01_test_pre</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          tag      testset  time_mean  score_mean  score_min  score_max  \\\n",
       "0  first-prog  01_test_pre         10         0.0          0          0   \n",
       "\n",
       "   time_max  \n",
       "0        21  "
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pre test\n",
    "result_sub_df = pd.merge(pre_seed_df, result_df, on='seed')\n",
    "summary_df = get_summary_df(result_sub_df)\n",
    "summary_df['testset'] = '01_test_pre'\n",
    "\n",
    "summary_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sys param\n",
    "if testset_name == '04_testset_param' or testset_name == '02_testset_sys' or testset_name == '03_testset_stress':\n",
    "    summary_df = pd.DataFrame()\n",
    "    \n",
    "    result_sub_df = pd.merge(param_seed_df, result_df, on='seed')\n",
    "    summary_df = get_summary_df(result_sub_df)\n",
    "    summary_df['testset'] = '04_test_param'\n",
    "\n",
    "summary_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sys test\n",
    "if testset_name == '02_testset_sys' or testset_name == '03_testset_stress':\n",
    "    summary_df = pd.DataFrame()\n",
    "    \n",
    "    result_sub_df = pd.merge(sys_seed_df, result_df, on='seed')\n",
    "    summary_df = get_summary_df(result_sub_df)\n",
    "    summary_df['testset'] = '02_test_sys'\n",
    "\n",
    "summary_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stress test\n",
    "if testset_name == '03_testset_stress':\n",
    "    summary_df = pd.DataFrame()\n",
    "    \n",
    "    result_sub_df = pd.merge(stress_seed_df, result_df, on='seed')\n",
    "    summary_df = get_summary_df(result_sub_df)\n",
    "    summary_df['testset'] = '03_test_stress'\n",
    "\n",
    "summary_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>testset</th>\n",
       "      <th>seed</th>\n",
       "      <th>time</th>\n",
       "      <th>score</th>\n",
       "      <th>result</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>01_testset_pre</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>dummy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>01_testset_pre</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>dummy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>01_testset_pre</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>dummy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>01_testset_pre</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>dummy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>01_testset_pre</td>\n",
       "      <td>70</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>dummy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>01_testset_pre</td>\n",
       "      <td>40</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>dummy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>01_testset_pre</td>\n",
       "      <td>39</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>dummy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>01_testset_pre</td>\n",
       "      <td>33</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>dummy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>01_testset_pre</td>\n",
       "      <td>48</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>dummy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>01_testset_pre</td>\n",
       "      <td>148</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>dummy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>300 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            testset seed time score result\n",
       "1    01_testset_pre    1    3     0  dummy\n",
       "3    01_testset_pre    3    3     0  dummy\n",
       "9    01_testset_pre    9    3     0  dummy\n",
       "10   01_testset_pre   10    3     0  dummy\n",
       "70   01_testset_pre   70    4     0  dummy\n",
       "..              ...  ...  ...   ...    ...\n",
       "40   01_testset_pre   40   19     0  dummy\n",
       "39   01_testset_pre   39   19     0  dummy\n",
       "33   01_testset_pre   33   19     0  dummy\n",
       "48   01_testset_pre   48   20     0  dummy\n",
       "148  01_testset_pre  148   21     0  dummy\n",
       "\n",
       "[300 rows x 5 columns]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df.sort_values('time', ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#result_df.sort_values('rel_score', ascending=True).head(n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 結果ログの保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_now = datetime.datetime.now() + datetime.timedelta(hours=9)\n",
    "time_str = t_now.strftime('%Y%m%d_%H%M')\n",
    "\n",
    "for PROG_NAME in PROG_NAME_LIST:\n",
    "    result_df = result_dict[PROG_NAME]\n",
    "    \n",
    "    for testset in np.unique(result_df['testset']):\n",
    "        csv_df = result_df.query('testset == \"{}\"'.format(testset))\n",
    "        csv_df.to_csv(PRJ_DIR+'/result/{}_{}_{}_{}.csv'.format(time_str,PROG_TAG, testset, PROG_NAME), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
